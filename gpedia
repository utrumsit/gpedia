#!/usr/bin/env python3
"""
gpedia - CLI for Grokipedia lookups

Usage:
    gpedia <topic>              Look up a topic
    gpedia <topic> | less       Pipe through pager
    gpedia <topic> --json       Output raw JSON
    gpedia <topic> --refs       Include references
    gpedia <topic> --short      Truncate to ~500 words
"""

import argparse
import json
import re
import sys
import textwrap
from urllib.parse import quote
from urllib.request import urlopen, Request
from urllib.error import HTTPError, URLError

API_BASE = "https://grokipedia-api.vercel.app"

# ANSI colors (disabled if not a tty or NO_COLOR set)
def supports_color():
    import os
    if os.environ.get("NO_COLOR"):
        return False
    if not sys.stdout.isatty():
        return False
    return True

COLORS = supports_color()

BOLD = "\033[1m" if COLORS else ""
DIM = "\033[2m" if COLORS else ""
ITALIC = "\033[3m" if COLORS else ""
CYAN = "\033[36m" if COLORS else ""
YELLOW = "\033[33m" if COLORS else ""
GREEN = "\033[32m" if COLORS else ""
MAGENTA = "\033[35m" if COLORS else ""
RESET = "\033[0m" if COLORS else ""


def slugify(topic: str) -> str:
    """Convert topic to URL slug format."""
    # Replace spaces with underscores, lowercase
    slug = topic.strip().replace(" ", "_")
    # URL encode for safety
    return quote(slug, safe="_")


def fetch_page(slug: str, refs: bool = False, truncate: int = None) -> dict:
    """Fetch page from Grokipedia API."""
    url = f"{API_BASE}/page/{slug}?extract_refs=true"
    if refs:
        url += "&citations=true"
    if truncate:
        url += f"&truncate={truncate}"

    req = Request(url, headers={"User-Agent": "gpedia-cli/1.0"})

    try:
        with urlopen(req, timeout=30) as response:
            return json.loads(response.read().decode("utf-8"))
    except HTTPError as e:
        if e.code == 404:
            return None
        raise
    except URLError as e:
        print(f"{YELLOW}Error:{RESET} Could not connect to API: {e.reason}", file=sys.stderr)
        sys.exit(1)


def clean_content(text: str, title: str) -> str:
    """Clean up API content: remove metadata, fix orphaned terms, detect headers."""
    lines = text.split("\n")

    # First pass: remove metadata before title
    start_idx = 0
    for i, line in enumerate(lines):
        if line.strip() == title:
            start_idx = i + 1
            break
    lines = lines[start_idx:]

    # Second pass: merge orphaned inline terms
    # Pattern: "sentence ending with\n\norphan_term\n\n—continuation or (continuation"
    merged = []
    i = 0
    while i < len(lines):
        line = lines[i].strip()

        # Skip blanks for now
        if not line:
            merged.append("")
            i += 1
            continue

        # Check if this is an orphan: short, between blanks, followed by continuation
        is_short = len(line) < 50 and len(line.split()) <= 4
        prev_blank = i > 0 and not lines[i - 1].strip()

        # Look ahead past blanks
        next_content_idx = i + 1
        while next_content_idx < len(lines) and not lines[next_content_idx].strip():
            next_content_idx += 1

        if is_short and prev_blank and next_content_idx < len(lines):
            next_line = lines[next_content_idx].strip()
            # Continuation: starts with lowercase, em-dash, paren, or period
            starts_continuation = next_line and next_line[0] in 'abcdefghijklmnopqrstuvwxyz—(-.,'

            if starts_continuation and not line.endswith(('.', '!', '?', ':')):
                # Find last non-blank in merged and append orphan + continuation
                for j in range(len(merged) - 1, -1, -1):
                    if merged[j]:
                        # Append the orphan term (italicized) and the continuation
                        # If continuation is just punctuation, no space before it
                        if next_line and next_line[0] in '.,:;!?)':
                            merged[j] = merged[j] + f" {ITALIC}{line}{RESET}" + next_line
                        else:
                            merged[j] = merged[j] + f" {ITALIC}{line}{RESET} " + next_line
                        break
                # Skip past the orphan and the blanks and continuation we just merged
                i = next_content_idx + 1
                continue

        merged.append(line)
        i += 1

    # Third pass: detect section headers
    result = []
    for i, line in enumerate(merged):
        if not line:
            result.append("")
            continue

        # Skip lines with ANSI codes (already formatted orphans)
        if '\033[' in line:
            result.append(line)
            continue

        prev_blank = i > 0 and not merged[i - 1]

        # Section header: preceded by blank, Title Case, multiple words, no ending punct
        words = line.split()
        is_header = (
            prev_blank and
            len(line) < 70 and
            len(words) >= 2 and
            line[0].isupper() and
            not line.endswith(('.', ',', ';', ':')) and
            sum(1 for w in words if w and w[0].isupper()) >= len(words) * 0.5
        )

        if is_header:
            result.append(f"\n{BOLD}{MAGENTA}{line}{RESET}")
        else:
            result.append(line)

    return "\n".join(result)


def wrap_text(text: str, width: int = 80) -> str:
    """Wrap text preserving paragraphs and formatting."""
    # Split on double newlines (paragraphs) and formatted headers
    paragraphs = re.split(r'\n\s*\n', text)
    wrapped = []

    for para in paragraphs:
        para = para.strip()
        if not para:
            continue

        # Check if it's a header (contains MAGENTA color code)
        if '\033[35m' in para or (MAGENTA and MAGENTA in para):
            wrapped.append(para)
            continue

        # Join lines within paragraph, preserving inline formatting
        lines = para.split("\n")
        joined = " ".join(line.strip() for line in lines if line.strip())

        if joined:
            # Wrap but be careful with ANSI codes
            # Simple approach: wrap the text, codes will flow with their words
            wrapped.append(textwrap.fill(joined, width=width))

    return "\n\n".join(wrapped)


def format_references(refs: list) -> str:
    """Format references section."""
    if not refs:
        return ""

    lines = [f"\n{BOLD}References{RESET}", "─" * 40]
    for ref in refs:
        num = ref.get("number", "?")
        url = ref.get("url", "")
        if url:
            lines.append(f"{DIM}[{num}]{RESET} {url}")
        else:
            lines.append(f"{DIM}[{num}]{RESET} (no URL)")
    return "\n".join(lines)


def format_output(page: dict, show_refs: bool = False) -> str:
    """Format page for terminal display."""
    title = page.get("title", "Unknown")
    content = page.get("content_text", "")
    url = page.get("url", "")
    word_count = page.get("word_count", 0)
    refs = page.get("references", [])

    # Header
    output = []
    output.append(f"{BOLD}{CYAN}{title}{RESET}")
    output.append(f"{DIM}{url}{RESET}")
    output.append(f"{DIM}{word_count} words{RESET}")
    output.append("═" * min(len(title) + 4, 80))
    output.append("")

    # Clean up content (remove metadata, fix orphans, detect headers)
    cleaned = clean_content(content, title)

    # Wrap for readability
    wrapped = wrap_text(cleaned, width=78)
    output.append(wrapped)

    # References
    if show_refs and refs:
        output.append(format_references(refs))

    return "\n".join(output)


def main():
    parser = argparse.ArgumentParser(
        description="Look up topics on Grokipedia",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  gpedia "Albert Einstein"
  gpedia stoicism --refs
  gpedia "quantum mechanics" --short
  gpedia philosophy | less
  gpedia "roman empire" --json | jq .word_count
        """
    )
    parser.add_argument("topic", nargs="+", help="Topic to look up")
    parser.add_argument("--json", action="store_true", help="Output raw JSON")
    parser.add_argument("--refs", action="store_true", help="Include references")
    parser.add_argument("--short", action="store_true", help="Truncate output (~500 words)")
    parser.add_argument("--truncate", type=int, metavar="N", help="Truncate to N characters")

    args = parser.parse_args()

    # Join topic words
    topic = " ".join(args.topic)
    slug = slugify(topic)

    # Determine truncation
    truncate = None
    if args.short:
        truncate = 3000  # ~500 words
    elif args.truncate:
        truncate = args.truncate

    # Fetch
    page = fetch_page(slug, refs=args.refs, truncate=truncate)

    if page is None:
        print(f"{YELLOW}Not found:{RESET} No page for '{topic}'", file=sys.stderr)
        print(f"{DIM}Try a different spelling or topic{RESET}", file=sys.stderr)
        sys.exit(1)

    # Output
    if args.json:
        print(json.dumps(page, indent=2))
    else:
        print(format_output(page, show_refs=args.refs))


if __name__ == "__main__":
    main()
